main:
  - title: "Timer-XL: Long-Context Transformers for Unified Time Series Forecasting"
    authors: <strong>Yong Liu*</strong>, Guo Qin*, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: arXiv
    conference: arXiv preprint, 2024.
    pdf: https://arxiv.org/abs/2410.04803
    image: ./assets/img/timer_xl_cover.png

  - title: "Timer: Generative Pre-trained Transformers Are Large Time Series Models"
    authors: <strong>Yong Liu*</strong>, Haoran Zhang*, Chenyu Li*, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: ICML
    conference: International Conference on Machine Learning, 2024.
    pdf: https://arxiv.org/abs/2402.02368
    code: https://github.com/thuml/Large-Time-Series-Model
    image: ./assets/img/timer_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/91da8a3d06984f209461/
    slides: https://cloud.tsinghua.edu.cn/f/b766629dbc584a4e8563/
    dataset: https://huggingface.co/datasets/thuml/UTSD
    notes: Accepted

  - title: "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models"
    authors: <strong>Yong Liu*</strong>, Guo Qin*, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2024.
    pdf: https://arxiv.org/abs/2402.02370
    code: https://github.com/thuml/AutoTimes
    image: ./assets/img/autotimes_cover.png
    slides: https://cloud.tsinghua.edu.cn/f/7689d30f92594ded84f0/
    notes: Accepted

  - title: "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"
    authors: <strong>Yong Liu*</strong>, Tengge Hu*, Haoran Zhang*, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long#
    conference_short: ICLR
    conference: International Conference on Learning Representations, 2024.
    pdf: https://arxiv.org/abs/2310.06625
    code: https://github.com/thuml/iTransformer
    image: ./assets/img/itransformer_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/36a2ae6c132d44c0bd8c/
    slides: https://cloud.tsinghua.edu.cn/f/175ff98f7e2d44fbbe8e/
    blog: https://mp.weixin.qq.com/s/-pvBnA1_NSloNxa6TYXTSg
    notes: Spotlight

  - title: "TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables"
    authors: Yuxuan Wang*, Haixu Wu*, Jiaxiang Dong, <strong>Yong Liu</strong>, Yunzhong Qiu, Haoran Zhang, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2024.
    pdf: https://arxiv.org/abs/2402.19072
    image: ./assets/img/timexer_cover.png
    notes: Accepted

  - title: "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis"
    authors: Haiwu Wu*, Tengge Hu*, <strong>Yong Liu*</strong>, Hang Zhou, Jianmin Wang, Mingsheng Long#
    conference_short: ICLR
    conference: International Conference on Learning Representations, 2023.
    pdf: https://arxiv.org/abs/2210.02186
    code: https://github.com/thuml/TimesNet
    image: ./assets/img/timesnet_cover.png
    slides: https://wuhaixu2016.github.io/pdf/ICLR2023_TimesNet.pdf
    blog: https://mp.weixin.qq.com/s/dhk-ASnrNG_Y99xykHaCUA
    notes: Accepted

  - title: "Deep Time Series Models: A Comprehensive Survey and Benchmark"
    authors: Yuxuan Wang*, Haixu Wu*, Jiaxiang Dong, <strong>Yong Liu</strong>, Mingsheng Long, Jianmin Wang#
    conference_short: arXiv
    conference: arXiv preprint, 2024.
    pdf: https://arxiv.org/abs/2407.13278
    image: ./assets/img/dl4ts_cover.png

  - title: "Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors"
    authors: <strong>Yong Liu*</strong>, Chenyu Li*, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2023.
    pdf: https://arxiv.org/abs/2305.18803
    code: https://github.com/thuml/Koopa
    image: ./assets/img/koopa_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/2f2fc7bd87d340ffaf29/
    slides: https://cloud.tsinghua.edu.cn/f/407ef231c6cb4727a6fa/
    blog: https://mp.weixin.qq.com/s/10PoA6n51Qok-nJT6_vkhA
    notes: Accepted

  - title: "Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting"
    authors: <strong>Yong Liu*</strong>, Haiwu Wu*, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2022.
    pdf: https://arxiv.org/abs/2205.14415
    code: https://github.com/thuml/Nonstationary_Transformers
    image: ./assets/img/nsformer_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/6eea66909aa7465ca9a4/
    slides: https://cloud.tsinghua.edu.cn/f/8d6ce7b18d3c468190e7/
    blog: https://mp.weixin.qq.com/s/LkpkTiNBVBYA-FqzAdy4dw
    notes: Accepted

  - title: "Ranking and Tuning Pre-trained Models: A New Paradigm for Exploiting Model Hubs"
    authors: Kaichao You*, <strong>Yong Liu*</strong>, Ziyang Zhang, Jianmin Wang, Michael I. Jordan, Mingsheng Long#
    conference_short: JMLR
    conference: Journal of Machine Learning Research, 2022.
    pdf: https://arxiv.org/abs/2110.10545v4
    code: https://github.com/thuml/LogME
    image: ./assets/img/btuning_cover.png
    blog: https://mp.weixin.qq.com/s/fgmp5Cph8wIgf2IbPoHj7w
    notes: Accepted

  - title: "LogME: Practical Assessment of Pre-trained Models for Transfer Learning"
    authors: Kaichao You*, <strong>Yong Liu*</strong>, Jianmin Wang, Mingsheng Long#
    conference_short: ICML
    conference: International Conference on Machine Learning, 2021.
    pdf: https://proceedings.mlr.press/v139/you21b.html
    code: https://github.com/thuml/LogME
    image: ./assets/img/logme_cover.png
    blog: https://mp.weixin.qq.com/s/9lJEcwkXAN4jaENNghjpyw
    notes: Accepted